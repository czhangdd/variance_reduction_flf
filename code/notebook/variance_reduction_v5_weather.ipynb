{"cells":[{"cell_type":"code","source":["dbutils.library.installPyPI('snowflake-connector-python', version = \"2.0.2\")\ndbutils.library.installPyPI(\"azure-storage-blob\", version = \"2.1.0\")\ndbutils.library.installPyPI(\"lightgbm\")\ndbutils.library.installPyPI(\"scikit-learn\", version=\"0.18.2\")\ndbutils.library.installPyPI('numpy', version=\"1.18.1\")\ndbutils.library.installPyPI(\"dill\", version=\"0.2.9\")\ndbutils.library.installPyPI(\"mlflow\")\n# dbutils.library.installPyPI(\"matplotlib\", version = \"1.5.3\")\ndbutils.library.installPyPI(\"category_encoders\", version=\"2.2.2\")\ndbutils.library.installPyPI(\"pandas\", version=\"0.21.0\")\n\n\ndbutils.library.restartPython()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import pandas as pd\nimport os\nfrom os import path\nfrom scipy.stats import pearsonr, spearmanr\nfrom scipy import stats\nimport numpy as np\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.metrics import log_loss, accuracy_score, average_precision_score, confusion_matrix, f1_score, roc_auc_score, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport snowflake.connector\nimport dill\nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import TargetEncoder\nimport mlflow"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["if mlflow.get_tracking_uri() != 'databricks':\n  print('update tracking uri')\n  mlflow.set_tracking_uri('databricks')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["def _get_key_metrics(y, y_pred, y_pred_proba):\n  accuracy = accuracy_score(y, y_pred)\n  roc_auc = roc_auc_score(y, y_pred_proba)\n  pr_auc = average_precision_score(y, y_pred)\n  logloss = log_loss(y, y_pred_proba)\n  mae = mean_absolute_error(y, y_pred)\n  conf_max = confusion_matrix(y, y_pred)/len(y)\n\n  return accuracy, roc_auc, pr_auc, logloss, mae\n  \n  \ndef eval_model(y_train, y_train_pred, y_train_pred_proba, y_test, y_test_pred, y_test_pred_proba): \n  y_train = np.reshape(y_train.astype('float').values, (len(y_train), ))\n  y_test = np.reshape(y_test.astype('float').values, (len(y_test), ))\n  \n  print('in y_train, pct of ones', y_train.mean(), 'pct of zeros', 1-y_train.mean())  \n  print('in y_test, pct of ones', y_test.mean(), 'pct of zeros', 1-y_test.mean())\n  \n  residual_std_train = (y_train_pred_proba - y_train).std()\n  y_train_std = y_train.std()\n  residual_std_test = (y_test_pred_proba - y_test).std()\n  y_test_std = y_test.std()\n  \n  accuracy_train, roc_auc_train, pr_auc_train, log_loss_train, mae_train = _get_key_metrics(y_train, y_train_pred, y_train_pred_proba)\n  accuracy_test, roc_auc_test, pr_auc_test, log_loss_test, mae_test = _get_key_metrics(y_test, y_test_pred, y_test_pred_proba)\n  \n  std_shrinking_test = (residual_std_test - y_test_std)/y_test_std\n  std_shrinking_train = (residual_std_test - y_test_std)/y_test_std\n  \n#     logloss_test_pred = log_loss(y_test_reshaped, y_pred_proba)\n#   logloss_baseline = log_loss(y_test_reshaped, np.ones(len(y_pred)))\n#   pred_random = np.random.randint(2, size=len(y_test_reshaped))\n#   logloss_baseline = log_loss(y_test_reshaped, pred_random)\n#   print('logloss_baseline', logloss_baseline)\n#   print('logloss_test_pred', logloss_test_pred)\n  \n#   acc = accuracy_score(y_test_reshaped, y_pred)\n#   f1 = f1_score(y_test_reshaped, y_pred)  \n#   roc_auc = roc_auc_score(y_test_reshaped, y_pred_proba)\n#   print('acc', acc)\n#   print('average_precision_score predicted', average_precision_score(y_test_reshaped, y_pred_proba), 'average_precision_score all zeros', average_precision_score(y_test_reshaped, np.zeros(len(y_pred))))\n#   print('f1 score', f1)\n#   print('roc_auc_score', roc_auc)\n  \n    \n  return std_shrinking_test, std_shrinking_train, \\\n         residual_std_test, y_test_std, residual_std_train, y_train_std, \\\n         accuracy_test, roc_auc_test, pr_auc_test, log_loss_test, mae_test, \\\n         accuracy_train, roc_auc_train, pr_auc_train, log_loss_train, mae_train"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["#### global variables"],"metadata":{}},{"cell_type":"code","source":["number_of_chunks = 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Data loading"],"metadata":{}},{"cell_type":"markdown","source":["#### Real-time query snowflake using complicated sql"],"metadata":{}},{"cell_type":"code","source":["sql_query_train = \"\"\"\nwith daypart_mapping as (\n  SELECT \n    case when day_part = 'late_night' then 'latenight' \n      else DAY_PART\n    END as daypart,\n    MIN(LOCAL_HOUR) as min_hour,\n    MAX(LOCAL_HOUR) as max_hour\n  FROM PRODDB.STATIC.LOOKUP_DAY_PART_MAPPING \n  GROUP BY 1\n),\nflf_targets as (\nSELECT \n  FLF.STARTING_POINT_ID, \n  FLF.STARTING_POINT_NAME, \n  FLF.TIME_OF_DAY as daypart, \n  dp.min_hour,\n  dp.max_hour,\n  FLF.TARGET_IDEAL_FLF, \n  FLF.MIN_TARGET_FLF_RANGE, \n  FLF.MAX_TARGET_FLF_RANGE, \n  FLF.TARGET_CREATED_AT, \n  LEAD(FLF.TARGET_CREATED_AT, 1) OVER (PARTITION BY FLF.STARTING_POINT_ID, FLF.TIME_OF_DAY ORDER BY FLF.TARGET_CREATED_AT) as next, \n  IFNULL(next, '2022-01-01') AS NEXT_TARGET_CREATED_DATE\nFROM STATIC.LOOKUP_TARGET_FLF_BY_REGION flf\nLEFT JOIN daypart_mapping dp \n  on flf.TIME_OF_DAY = dp.DAYPART\n),\nflf_raw as (\nSELECT\n  dd.created_at,\n  dd.DELIVERY_ID,\n  dd.active_date,\n  dd.STORE_STARTING_POINT_ID,\n  dd.SUBMARKET_ID,\n  dd.flf,\n  fces.num_delivered as num_delivered,\n  fces.num_opened as num_opened,\n  sm.LAUNCH_DATE as submarket_launch_date,\n  convert_timezone('UTC', dd.TIMEZONE, dd.CREATED_AT) as created_at_local,\n  TO_DATE(created_at_local) as created_at_local_date,\n  hour(created_at_local) * 2 + floor(minute(created_at_local)/30.0) as window_id,\n  datediff('second', dd.CREATED_AT, dd.ACTUAL_DELIVERY_TIME)/60.0 as asap,\n  dd.DISTINCT_ACTIVE_DURATION/60.0 as dat,\n  datediff('second', dd.DASHER_CONFIRMED_TIME, dd.DASHER_AT_STORE_TIME)/60.0 as d2r,\n  case when datediff('second', dd.QUOTED_DELIVERY_TIME, dd.ACTUAL_DELIVERY_TIME)/60 > 20 then 1 else 0 END as lateness_20_min,\n  flf.daypart,\n  case when dd.flf - flf.MAX_TARGET_FLF_RANGE > 0 then 1 else 0 END as is_flf_above_max,\n  case when dd.flf - flf.TARGET_IDEAL_FLF > 0 then 1 else 0 END as is_flf_above_ideal\nFROM PRODDB.PUBLIC.DIMENSION_DELIVERIES dd \nLEFT JOIN flf_targets flf\n  on dd.STORE_STARTING_POINT_ID = flf.STARTING_POINT_ID\n  AND hour(convert_timezone('UTC', dd.TIMEZONE, dd.created_at)) between flf.min_hour and flf.max_hour\n  AND convert_timezone('UTC', dd.TIMEZONE, dd.created_at) between flf.TARGET_CREATED_AT and flf.NEXT_TARGET_CREATED_DATE\nLEFT JOIN PRODDB.PUBLIC.MAINDB_SUBMARKET sm \n  ON dd.SUBMARKET_ID = sm.ID\nLEFT JOIN public.fact_cx_email_summary fces\n  ON dd.SUBMARKET_ID = fces.SUBMARKET_ID\n  AND convert_timezone('UTC',dd.timezone,\n dateadd('minute',cast(floor(date_part('minute',dd.created_at) / 30) * 30 as int), date_trunc('hour',dd.created_at))\n ) = fces.half_hour_local\nWHERE dd.created_at between '2020-03-16' and '2020-04-27'\n  AND dd.IS_FILTERED_CORE = true \n  AND dd.IS_ASAP = true \n  AND dd.IS_CONSUMER_PICKUP = false \n  AND fulfillment_type != 'merchant_fleet'\n),\n\nflf_raw_grouped as(\nSELECT\n    t.created_at_local_date,\n    t.DAYPART,\n    t.STORE_STARTING_POINT_ID,\n    AVG(t.ASAP) as avg_asap,\n    AVG(t.DAT) as avg_dat,\n    AVG(t.D2R) as avg_d2r,\n    AVG(t.IS_FLF_ABOVE_IDEAL) as avg_is_flf_above_ideal,\n    AVG(t.flf) as avg_flf,\n    AVG(t.num_opened) as avg_num_opened,\n    AVG(t.num_delivered) as avg_num_delivered\nFROM flf_raw t\nGROUP BY t.created_at_local_date, t.DAYPART, t.STORE_STARTING_POINT_ID\n),\nflf_hist as(\nSELECT \n    t1.created_at_local_date,\n    dayofweek(t1.CREATED_AT_LOCAL) as DAY_OF_WEEK,\n    hour(t1.CREATED_AT_LOCAL) as HOUR_OF_DAY,\n    t1.STORE_STARTING_POINT_ID,\n    t1.SUBMARKET_ID,  \n    t1.DAYPART,\n    t1.WINDOW_ID,\n    t1.is_flf_above_ideal\nFROM flf_raw t1\nLEFT JOIN flf_raw_grouped t2\n    ON t1.created_at_local_date = DATEADD(Day, -15, t2.created_at_local_date)\n    AND t1.DAYPART = t2.DAYPART\n    AND t1.STORE_STARTING_POINT_ID = t2.STORE_STARTING_POINT_ID\n)\n\nSELECT *\nFROM flf_hist\nSAMPLE(20)\n\"\"\"\n\n# sql_query_test_verify = \"\"\"\n# CREATE TEMP TABLE CHIZHANG.DAYPART_MAPPING AS (\n#   SELECT \n#     CASE WHEN day_part = 'late_night' THEN 'latenight' \n#       ELSE DAY_PART\n#     END AS daypart,\n#     MIN(LOCAL_HOUR) AS min_hour,\n#     MAX(LOCAL_HOUR) AS max_hour\n#   FROM PRODDB.STATIC.LOOKUP_DAY_PART_MAPPING \n#   GROUP BY 1\n# );\n\n# CREATE temp TABLE CHIZHANG.FLF_TARGETS AS (\n# SELECT \n#   FLF.STARTING_POINT_ID, \n#   FLF.STARTING_POINT_NAME, \n#   FLF.TIME_OF_DAY AS daypart, \n#   dp.min_hour,\n#   dp.max_hour,\n#   FLF.TARGET_IDEAL_FLF, \n#   FLF.MIN_TARGET_FLF_RANGE, \n#   FLF.MAX_TARGET_FLF_RANGE, \n#   FLF.TARGET_CREATED_AT, \n#   LEAD(FLF.TARGET_CREATED_AT, 1) OVER (PARTITION BY FLF.STARTING_POINT_ID, FLF.TIME_OF_DAY ORDER BY FLF.TARGET_CREATED_AT) AS NEXT, \n#   IFNULL(NEXT, '2022-01-01') AS NEXT_TARGET_CREATED_DATE\n# FROM STATIC.LOOKUP_TARGET_FLF_BY_REGION flf\n# LEFT JOIN CHIZHANG.DAYPART_MAPPING dp \n#   ON flf.TIME_OF_DAY = dp.DAYPART\n# );\n    \n# CREATE temp TABLE CHIZHANG.FLF_RAW AS (\n# SELECT\n#   dd.created_at,\n#   dd.DELIVERY_ID,\n#   dd.active_date,\n#   dd.STORE_STARTING_POINT_ID,\n#   dd.SUBMARKET_ID,\n#   dd.flf,\n#   fces.num_delivered AS num_delivered,\n#   fces.num_opened AS num_opened,\n#   sm.LAUNCH_DATE AS submarket_launch_date,\n#   convert_timezone('UTC', dd.TIMEZONE, dd.CREATED_AT) AS created_at_local,\n#   TO_DATE(created_at_local) AS created_at_local_date,\n#   hour(created_at_local) * 2 + floor(minute(created_at_local)/30.0) AS window_id,\n#   datediff('second', dd.CREATED_AT, dd.ACTUAL_DELIVERY_TIME)/60.0 AS asap,\n#   dd.DISTINCT_ACTIVE_DURATION/60.0 AS dat,\n#   datediff('second', dd.DASHER_CONFIRMED_TIME, dd.DASHER_AT_STORE_TIME)/60.0 as d2r,\n#   CASE WHEN datediff('second', dd.QUOTED_DELIVERY_TIME, dd.ACTUAL_DELIVERY_TIME)/60 > 20 THEN 1 ELSE 0 END AS lateness_20_min,\n#   flf.daypart,\n#   CASE WHEN dd.flf - flf.MAX_TARGET_FLF_RANGE > 0 THEN 1 ELSE 0 END AS is_flf_above_max,\n#   CASE WHEN dd.flf - flf.TARGET_IDEAL_FLF > 0 THEN 1 ELSE 0 END AS is_flf_above_ideal\n# FROM PRODDB.PUBLIC.DIMENSION_DELIVERIES dd \n# LEFT JOIN CHIZHANG.FLF_TARGETS flf\n#   ON dd.STORE_STARTING_POINT_ID = flf.STARTING_POINT_ID\n#   AND hour(convert_timezone('UTC', dd.TIMEZONE, dd.created_at)) BETWEEN flf.min_hour AND flf.max_hour\n#   AND convert_timezone('UTC', dd.TIMEZONE, dd.created_at) BETWEEN flf.TARGET_CREATED_AT AND flf.NEXT_TARGET_CREATED_DATE\n# LEFT JOIN PRODDB.PUBLIC.MAINDB_SUBMARKET sm \n#   ON dd.SUBMARKET_ID = sm.ID\n# LEFT JOIN public.fact_cx_email_summary fces\n#   ON dd.SUBMARKET_ID = fces.SUBMARKET_ID\n#   AND convert_timezone('UTC',dd.timezone,\n#  dateadd('minute',CAST(floor(date_part('minute',dd.created_at) / 30) * 30 AS INT), date_trunc('hour',dd.created_at))\n#  ) = fces.half_hour_local\n# //  WHERE CAST(DD.CREATED_AT as DATE) >= dateadd('DAY', -3, TO_TIMESTAMP_NTZ(LOCALTIMESTAMP)) \n#   WHERE dd.created_at BETWEEN '2020-03-16' AND '2020-04-27'\n#   AND CAST(DD.CREATED_AT as DATE) < dateadd('DAY', 0, TO_TIMESTAMP_NTZ(LOCALTIMESTAMP))  \n#   AND dd.IS_FILTERED_CORE = true \n#   AND dd.IS_ASAP = true \n#   AND dd.IS_CONSUMER_PICKUP = false \n#   AND fulfillment_type != 'merchant_fleet'\n# );\n\n# CREATE TEMP TABLE CHIZHANG.FLF_RAW_GROUPED AS(\n# SELECT\n#     t.created_at_local_date,\n#     t.DAYPART,\n#     t.STORE_STARTING_POINT_ID,\n#     AVG(t.ASAP) AS avg_asap,\n#     AVG(t.DAT) AS avg_dat,\n#     AVG(t.D2R) AS avg_d2r,\n#     AVG(t.IS_FLF_ABOVE_IDEAL) AS avg_is_flf_above_ideal,\n#     AVG(t.flf) AS avg_flf,\n#     AVG(t.num_opened) AS avg_num_opened,\n#     AVG(t.num_delivered) AS avg_num_delivered\n# FROM CHIZHANG.FLF_RAW t\n# GROUP BY t.created_at_local_date, t.DAYPART, t.STORE_STARTING_POINT_ID\n# );\n\n# SELECT \n#     -- t1.CREATED_AT,\n#     -- t1.DELIVERY_ID, \n#     -- t1.ACTIVE_DATE, \n#     dayofweek(t1.CREATED_AT_LOCAL) as DAY_OF_WEEK,\n#     hour(t1.CREATED_AT_LOCAL) as HOUR_OF_DAY,\n#     t1.STORE_STARTING_POINT_ID,\n#     t1.SUBMARKET_ID,\n#     -- t1.FLF,\n#     -- t1.NUM_DELIVERED, \n#     -- t1.NUM_OPENED,\n#     -- t1.SUBMARKET_LAUNCH_DATE, \n#     -- t1.CREATED_AT_LOCAL, \n#     -- t1.CREATED_AT_LOCAL_DATE,\n#     -- t1.ASAP, \n#     -- t1.DAT,\n#     -- t1.D2R, \n#     -- t1.LATENESS_20_MIN,\n#     t1.DAYPART,\n#     t1.WINDOW_ID\n#     -- t1.IS_FLF_ABOVE_MAX, \n#     -- t1.IS_FLF_ABOVE_IDEAL,\n#     -- t2.avg_asap,\n#     -- t2.avg_dat,\n#     -- t2.avg_d2r,\n#     -- t2.avg_is_flf_above_ideal,\n#     -- t2.avg_flf,\n#     -- t2.avg_num_delivered,\n#     -- t2.avg_num_opened\n# FROM CHIZHANG.FLF_RAW t1\n# LEFT JOIN CHIZHANG.FLF_RAW_GROUPED t2\n#     ON t1.created_at_local_date = DATEADD(Day, -15, t2.created_at_local_date)\n#     AND t1.DAYPART = t2.DAYPART\n#     AND t1.STORE_STARTING_POINT_ID = t2.STORE_STARTING_POINT_ID\n# SAMPLE(5)\n# \"\"\"\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# full query. complicated query from snowflake\n# toPandas is time-consuming\nscope_name = 'chizhang-scope'\npw_key_name = 'snowflake-password'\nun_key_name = 'snowflake-user'\nuser = dbutils.secrets.get(scope=scope_name, key=un_key_name)\npassword = dbutils.secrets.get(scope=scope_name, key=pw_key_name)\n\n# snowflake connection options\noptions = dict(sfurl=\"doordash.snowflakecomputing.com/\",\n               sfaccount=\"DOORDASH\",\n               sfuser=user,\n               sfpassword=password,\n               sfdatabase=\"PRODDB\",\n               sfschema=\"public\",\n               sfwarehouse=\"ADHOC\")\n\nprint(user)\nprint(password)\n\n# sql_load_table = \"\"\" select * from variance_reduction_flf_train_test \"\"\"\nall_data = spark.read.format(\"snowflake\").options(**options).option(\"query\", sql_query_train).load()\nraw_data = all_data.toPandas()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Query pre-generated table in snowflake using simple sql"],"metadata":{}},{"cell_type":"code","source":["sql_load_pre_gen_table = \"\"\"select * from flf_weather_supply_demand sample (20)\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# simple query.\n# load pre-generated table from snowflake\n\nuser = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-user\")\npassword = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-password\")\n\nos.environ['SNOWFLAKE_USER'] = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-user\")\nos.environ['SNOWFLAKE_PW'] = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-password\")\n\n# snowflake connection options\nparams = dict(\n  user=os.environ['SNOWFLAKE_USER'],\n  password=os.environ['SNOWFLAKE_PW'],\n  account='DOORDASH',\n  database='PRODDB',\n  warehouse='ADHOC',\n  schema='public',\n)\n\nsql_query_simple = \"\"\"select * from flf_weather_supply_demand sample (20)\"\"\"\nwith snowflake.connector.connect(**params) as ctx:\n  raw_data = pd.read_sql(sql_query_simple, ctx)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["#### save csv to dbfs"],"metadata":{}},{"cell_type":"code","source":["raw_data_mem_size = raw_data.info(memory_usage='deep')\nprint('raw_data_mem_size', raw_data_mem_size)\n\n#15min to save\n[df_i.to_csv('/dbfs/chizhang/variance_reduction/data/data_weather_supply_demand_{id}.csv'.format(id=id)) for id, df_i in  enumerate(np.array_split(raw_data, number_of_chunks))]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nRangeIndex: 13157755 entries, 0 to 13157754\nData columns (total 39 columns):\nCREATED_AT                   datetime64[ns]\nDELIVERY_ID                  int64\nACTIVE_DATE                  object\nSTORE_STARTING_POINT_ID      int64\nSUBMARKET_ID                 int64\nFLF                          object\nNUM_DELIVERED                float64\nNUM_OPENED                   float64\nSUBMARKET_LAUNCH_DATE        object\nCREATED_AT_LOCAL             datetime64[ns]\nCREATED_AT_LOCAL_DATE        object\nDAY_OF_WEEK                  int64\nHOUR_OF_DAY                  int64\nWINDOW_ID                    int64\nASAP                         float64\nDAT                          float64\nD2R                          float64\nCREATED_AT_HOUR              datetime64[ns]\nLATENESS_20_MIN              int64\nDAYPART                      object\nIS_FLF_ABOVE_MAX             int64\nIS_FLF_ABOVE_IDEAL           int64\nHH_TEMPERATURE               float64\nHH_APPARENT_TEMPERATURE      float64\nHH_PRESSURE                  float64\nHH_HUMIDITY                  float64\nHH_VISIBILITY                float64\nHH_WIND_SPEED                float64\nHH_CLOUD_COVER               float64\nHH_DEWPOINT                  float64\nHH_HOURLY_WEATHER_SUMMARY    object\nHH_PRECIP_INTENSITY          float64\nHH_PRECIP_PROBABILITY        float64\nHH_ICON                      object\nHH_PRECIP_ACCUMULATION       float64\nHH_PRECIP_TYPE               object\nPRED_DEMAND                  float64\nACTUAL_DEMAND                float64\nUNDER_PREDICTED_DEMAND       float64\ndtypes: datetime64[ns](3), float64(19), int64(9), object(8)\nmemory usage: 8.2 GB\nraw_data_mem_size None\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["#### load from dbfs"],"metadata":{}},{"cell_type":"code","source":["#2min to load 10 files\ndir = r'/dbfs/chizhang/variance_reduction/data' # use your path\n\nli = []\n\nfor id in range(number_of_chunks):\n  print('chunk id', id)\n  filename = dir + '/data_weather_'+str(id)+'.csv'\n  df = pd.read_csv(filename, index_col=None)\n  li.append(df)\n\nraw_data = pd.concat(li, axis=0, ignore_index=True)\nprint(raw_data.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">chunk id 0\nchunk id 1\nchunk id 2\nchunk id 3\nchunk id 4\nchunk id 5\nchunk id 6\nchunk id 7\nchunk id 8\nchunk id 9\n(11210012, 38)\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df = raw_data.copy()\ndf = df.reindex()\nprint('data size', df.shape)\ndf.columns = map(str.lower, df.columns)\ndf.head(5)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# drop cols with >50% na\nprint(df.shape)\ndf.columns\ndf_nan = df.isna().sum()/len(df)\n\ncols_to_remove = list(df_nan[df_nan > 0.5].index)\nprint('we drop cols with >50% missing values:', cols_to_remove)\ndf = df.drop(cols_to_remove, axis=1)\nprint(df.shape)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["df_dropped = df.dropna(inplace=False)\nprint(df_dropped.shape)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Feature creating"],"metadata":{}},{"cell_type":"code","source":["# combine real-time features with historical aggregated features\n# features_cat = ['day_of_week', 'hour_of_day', 'STORE_STARTING_POINT_ID', 'SUBMARKET_ID', 'LATENESS_20_MIN', 'DAYPART', '30min'] #30 min, sp, submarket, ->market level feats\n# historical aggregated features := avg(feat_values) over SP_ID/(DATE_LOCAL-15days)/DAYPART units (i.e., 14 days ago) \n# features_num = ['AVG_ASAP', 'AVG_DAT', 'AVG_D2R', 'AVG_FLF', 'AVG_IS_FLF_ABOVE_IDEAL']\n\n# features_cat = ['day_of_week', 'hour_of_day', 'store_starting_point_id', 'submarket_id', 'daypart', 'window_id'] #30 min, sp, submarket, ->market level feats\n# # features_num = ['AVG_ASAP', 'AVG_DAT', 'AVG_D2R', 'AVG_FLF', 'AVG_IS_FLF_ABOVE_IDEAL', 'AVG_NUM_DELIVERED', 'AVG_NUM_OPENED']\n# features_num = []\n\nfeatures_orig = ['day_of_week', 'hour_of_day', 'store_starting_point_id', 'submarket_id', 'daypart', 'window_id']\nfeatures_weather = ['hh_temperature', 'hh_apparent_temperature', 'hh_pressure', 'hh_humidity', 'hh_dewpoint', 'hh_visibility', \\\n                'hh_wind_speed', 'hh_cloud_cover', 'hh_dewpoint', 'hh_precip_intensity', 'hh_precip_probability']\nfeatures_supply_demand = ['pred_demand', 'actual_demand', 'under_predicted_demand']\n\nfeatures_type = {\"day_of_week\" : \"category\",\n        \"hour_of_day\" : \"category\",\n        'daypart'     : \"category\",\n        'hh_hourly_weather_summary': \"category\",\n        'hh_precip_type': \"category\",\n        'hh_icon'     : \"category\",\n        \n        'store_starting_point_id' : \"target_encode\", \n        'submarket_id' : \"target_encode\",   \n        'window_id'    : \"target_encode\",\n        \n        'hh_temperature' : \"numerical\", \n        'hh_apparent_temperature' : \"numerical\", \n        'hh_pressure'    : \"numerical\",\n        'hh_humidity'    : \"numerical\",\n        'hh_dewpoint'    : \"numerical\", \n        'hh_visibility'  : \"numerical\",\n        'hh_wind_speed'  : \"numerical\",\n        'hh_cloud_cover' : \"numerical\", \n        'hh_dewpoint'    : \"numerical\",\n        'hh_precip_intensity'   : \"numerical\",\n        'hh_precip_probability' : \"numerical\"\n       }\n\n# features_cat = ['day_of_week', 'hour_of_day', 'store_starting_point_id', 'submarket_id', 'daypart', 'window_id', \\\n#                'hh_hourly_weather_summary',  'hh_precip_type', 'hh_icon']\n# features_num = ['hh_temperature', 'hh_apparent_temperature', 'hh_pressure', 'hh_humidity', 'hh_dewpoint', 'hh_visibility', \\\n#                 'hh_wind_speed', 'hh_cloud_cover', 'hh_dewpoint', 'hh_precip_intensity', 'hh_precip_probability', \\\n#                 'pred_demand']\n\nfeatures_target_encode = ['store_starting_point_id', 'submarket_id', 'window_id'] #, 'hh_hourly_weather_summary', 'hh_icon']\n\nfeatures = features_cat + features_num\nfeatures = list(set(features) - set(cols_to_remove))\nmetrics = ['is_flf_above_ideal']\n\n\nfor f in features_type:\n  if features_type.get(f) == 'category' or features_type.get(f) == 'target_encode':\n    df[f] = df[f].astype('category')\n  elif features_type.get(f) == 'numerical':\n    df[f] = df[f].astype('float')\n  else:\n    raise('wrong feature type')\n    \n\nfor f in features_cat:\n  df[f] = df[f].astype('category')\nfor f in features_num:\n  df[f] = df[f].astype('float')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["## Eval raw correlation"],"metadata":{}},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>Index([&apos;unnamed: 0&apos;, &apos;created_at&apos;, &apos;delivery_id&apos;, &apos;active_date&apos;,\n       &apos;store_starting_point_id&apos;, &apos;submarket_id&apos;, &apos;flf&apos;, &apos;num_delivered&apos;,\n       &apos;num_opened&apos;, &apos;submarket_launch_date&apos;, &apos;created_at_local&apos;,\n       &apos;created_at_local_date&apos;, &apos;day_of_week&apos;, &apos;hour_of_day&apos;, &apos;window_id&apos;,\n       &apos;asap&apos;, &apos;dat&apos;, &apos;d2r&apos;, &apos;created_at_hour&apos;, &apos;lateness_20_min&apos;, &apos;daypart&apos;,\n       &apos;is_flf_above_max&apos;, &apos;is_flf_above_ideal&apos;, &apos;hh_temperature&apos;,\n       &apos;hh_apparent_temperature&apos;, &apos;hh_pressure&apos;, &apos;hh_humidity&apos;,\n       &apos;hh_visibility&apos;, &apos;hh_wind_speed&apos;, &apos;hh_cloud_cover&apos;, &apos;hh_dewpoint&apos;,\n       &apos;hh_hourly_weather_summary&apos;, &apos;hh_precip_intensity&apos;,\n       &apos;hh_precip_probability&apos;, &apos;hh_icon&apos;, &apos;hh_precip_type&apos;, &apos;pred_demand&apos;],\n      dtype=&apos;object&apos;)</div>"]}}],"execution_count":26},{"cell_type":"code","source":["te = TargetEncoder(cols=features_target_encode)\ndf_temp = te.fit_transform(df[features], df[metrics])\nprint(df_temp)\n\n# for f in df_te.columns:\n#   df[f + '_te'] = df_te[f]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">          window_id  submarket_id day_of_week hour_of_day  \\\n0          0.217840      0.251854           2          16   \n1          0.217840      0.251854           2          16   \n2          0.217840      0.251854           2          16   \n3          0.219907      0.251854           2          16   \n4          0.219907      0.251854           2          16   \n5          0.219907      0.251854           2          16   \n6          0.219907      0.251854           2          16   \n7          0.219907      0.251854           2          16   \n8          0.219907      0.251854           2          16   \n9          0.239473      0.136636           2          15   \n10         0.239473      0.136636           2          15   \n11         0.239473      0.136636           2          15   \n12         0.239473      0.136636           2          15   \n13         0.205561      0.136636           2          15   \n14         0.205561      0.136636           2          15   \n15         0.205561      0.136636           2          15   \n16         0.205561      0.136636           2          15   \n17         0.239473      0.136636           2          15   \n18         0.239473      0.136636           2          15   \n19         0.169675      0.283148           2          14   \n20         0.169675      0.283148           2          14   \n21         0.170419      0.283148           2          14   \n22         0.170419      0.283148           2          14   \n23         0.169675      0.041616           2          14   \n24         0.169675      0.280318           2          14   \n25         0.100959      0.374912           2          13   \n26         0.100959      0.374912           2          13   \n27         0.100959      0.374912           2          13   \n28         0.100959      0.374912           2          13   \n29         0.100959      0.374912           2          13   \n...             ...           ...         ...         ...   \n11209982   0.243277      0.320958           2          19   \n11209983   0.243277      0.320958           2          19   \n11209984   0.243277      0.320958           2          19   \n11209985   0.243277      0.320958           2          19   \n11209986   0.243277      0.320958           2          19   \n11209987   0.375326      0.267184           2          18   \n11209988   0.375326      0.267184           2          18   \n11209989   0.375326      0.267184           2          18   \n11209990   0.375326      0.267184           2          18   \n11209991   0.413346      0.267184           2          18   \n11209992   0.413346      0.267184           2          18   \n11209993   0.413346      0.267184           2          18   \n11209994   0.413346      0.267184           2          18   \n11209995   0.413346      0.267184           2          18   \n11209996   0.413346      0.267184           2          18   \n11209997   0.413346      0.267184           2          18   \n11209998   0.413346      0.267184           2          18   \n11209999   0.413346      0.267184           2          18   \n11210000   0.413346      0.267184           2          18   \n11210001   0.413346      0.267184           2          18   \n11210002   0.413346      0.267184           2          18   \n11210003   0.298844      0.218362           5          17   \n11210004   0.298844      0.218362           5          17   \n11210005   0.298844      0.218362           5          17   \n11210006   0.219907      0.234021           5          16   \n11210007   0.219907      0.234021           5          16   \n11210008   0.217840      0.234021           5          16   \n11210009   0.217840      0.234021           5          16   \n11210010   0.298844      0.517358           5          17   \n11210011   0.298844      0.517358           5          17   \n\n          store_starting_point_id daypart  \n0                        0.251854   snack  \n1                        0.251854   snack  \n2                        0.251854   snack  \n3                        0.251854   snack  \n4                        0.251854   snack  \n5                        0.251854   snack  \n6                        0.251854   snack  \n7                        0.251854   snack  \n8                        0.251854   snack  \n9                        0.136636   snack  \n10                       0.136636   snack  \n11                       0.136636   snack  \n12                       0.136636   snack  \n13                       0.136636   snack  \n14                       0.136636   snack  \n15                       0.136636   snack  \n16                       0.136636   snack  \n17                       0.136636   snack  \n18                       0.136636   snack  \n19                       0.353293   snack  \n20                       0.353293   snack  \n21                       0.353293   snack  \n22                       0.353293   snack  \n23                       0.041616   snack  \n24                       0.376963   snack  \n25                       0.353305   lunch  \n26                       0.353305   lunch  \n27                       0.353305   lunch  \n28                       0.353305   lunch  \n29                       0.353305   lunch  \n...                           ...     ...  \n11209982                 0.399045  dinner  \n11209983                 0.399045  dinner  \n11209984                 0.399045  dinner  \n11209985                 0.399045  dinner  \n11209986                 0.399045  dinner  \n11209987                 0.239093  dinner  \n11209988                 0.239093  dinner  \n11209989                 0.239093  dinner  \n11209990                 0.239093  dinner  \n11209991                 0.239093  dinner  \n11209992                 0.239093  dinner  \n11209993                 0.239093  dinner  \n11209994                 0.239093  dinner  \n11209995                 0.239093  dinner  \n11209996                 0.239093  dinner  \n11209997                 0.239093  dinner  \n11209998                 0.239093  dinner  \n11209999                 0.239093  dinner  \n11210000                 0.239093  dinner  \n11210001                 0.239093  dinner  \n11210002                 0.239093  dinner  \n11210003                 0.218362  dinner  \n11210004                 0.218362  dinner  \n11210005                 0.218362  dinner  \n11210006                 0.234021   snack  \n11210007                 0.234021   snack  \n11210008                 0.234021   snack  \n11210009                 0.234021   snack  \n11210010                 0.394682  dinner  \n11210011                 0.394682  dinner  \n\n[11210012 rows x 6 columns]\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["df_temp['is_flf_above_ideal'] = df[metrics]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["print(df_temp.corr()['is_flf_above_ideal'].abs().sort_values(ascending=False))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">is_flf_above_ideal         1.000000\nstore_starting_point_id    0.316322\nsubmarket_id               0.244716\nwindow_id                  0.203667\nName: is_flf_above_ideal, dtype: float64\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["# feat_corr =  ['hh_pressure', 'hh_humidity', 'hh_precip_intensity', 'pred_demand', 'store_starting_point_id', 'submarket_id', 'window_id']\n\n\n# for f in feat_corr:\n#   tmp = df.corr()\n#   print(tmp.values[0][1])\n# for f in feat_corr:\n#   print('feature', f)\n#   if f in features_target_encode:\n#     print('\\tTrain PearsonrResult', pearsonr(ppl_best.named_steps[\"target_encode\"].transform(X_train)[f].values, y_train.values.reshape(len(y_train), )))\n#   else:\n#     print('\\tTrain PearsonrResult', pearsonr(X_train[f], y_train.values.reshape(len(y_train), )))\n#   print('\\tTrain PearsonrResult', pearsonr(X_train_encoded[f].values, y_train.values.reshape(len(y_train), )))#, spearmanr(X_train_encoded[f], y_train))  \n#   print('\\tTest PearsonrResult', pearsonr(X_test_encoded[f].values, y_test.values.reshape(len(y_test), )))#, spearmanr(X_test_encoded[f], y_test))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Train/test data preparing"],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime, timedelta\n\ndf['created_at_local_date'] = pd.to_datetime(df['created_at_local_date'])\ndate_train_end = df['created_at_local_date'].max() - timedelta(weeks=1)\ndate_train_start = date_train_end - timedelta(weeks=5)\n\ntrain_index = df[(df['created_at_local_date'] < str(date_train_end)) & (df['created_at_local_date'] >= str(date_train_start))].index\ntest_index = df[df['created_at_local_date'] >= str(date_train_end)].index\n\nprint('training period:', df.loc[train_index]['created_at_local_date'].min(), df.loc[train_index]['created_at_local_date'].max())\nprint('testing period:', df.loc[test_index]['created_at_local_date'].min(), df.loc[test_index]['created_at_local_date'].max())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">training period: 2020-03-16 00:00:00 2020-04-19 00:00:00\ntesting period: 2020-04-20 00:00:00 2020-04-27 00:00:00\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# check train and test index do not intersects\nassert len(train_index.intersection(test_index))==0, 'data leakage'\n# check if training and testing data has 7 days a week\nassert df.loc[test_index]['day_of_week'].nunique() == 7, 'testing data does not have 7 days a week'\nassert df.loc[train_index]['day_of_week'].nunique() == 7, 'training data does not have 7 days a week'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["### Create pipeline & train"],"metadata":{}},{"cell_type":"markdown","source":["#### random search"],"metadata":{}},{"cell_type":"code","source":["# target encoding\nteppl = TargetEncoder(cols=features_target_encode)\n# clf\nclfppl = LGBMClassifier()\n\n# random search\nrandomParams = {\n    'clf__learning_rate': stats.uniform(0.01, 0.1), \n    'clf__num_leaves': stats.randint(16, 512),\n    'clf__boosting_type' : ['gbdt', 'dart', 'goss']    \n}\n\nppl = Pipeline([('target_encode', teppl), ('clf', clfppl)])\n\nrandom_search = RandomizedSearchCV(ppl, randomParams, n_jobs=-1, verbose=2, n_iter=100)\nrandom_search.fit(X_train, y_train)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["random_search.best_params_"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["ppl_best = random_search.best_estimator_"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### best model after randome search"],"metadata":{}},{"cell_type":"code","source":["teppl = TargetEncoder(cols=features_target_encode)\n\nclfppl = LGBMClassifier()\n\nppl_best = Pipeline([('target_encode', teppl), ('clf', clfppl)])\nppl_best.set_params(clf__learning_rate=0.09985392034627226, clf__boosting_type='gbdt', clf__num_leaves=486)\n\nppl_best.fit(X_train, y_train)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["#### clf vanilla"],"metadata":{}},{"cell_type":"code","source":["mlflow.set_experiment('/Shared/Experiments/variance_reduction_flf')\n\ntrain_mlflow(features)\ntrain_mlflow(features)\ndef train_mlflow(features):\n  with mlflow.start_run(run_name='weather_vanilla'):\n\n      X_train = df.loc[train_index, features]\n      y_train = df.loc[train_index, metrics]\n\n      X_test = df.loc[test_index, features]\n      y_test = df.loc[test_index, metrics]\n\n      print(\"x_train shape\", X_train.shape, 'y_train shape', y_train.shape)\n      print(\"x_test shape\", X_test.shape, 'y_test shape', y_test.shape)\n\n      teppl_vanilla = TargetEncoder(cols=features_target_encode)\n      clfppl_vanilla = LGBMClassifier()\n\n      ppl_vanilla = Pipeline([('target_encode_vanilla', teppl_vanilla), ('clf_vanilla', clfppl_vanilla)])\n\n      ppl_vanilla.fit(X_train, y_train)\n\n      y_test_pred = ppl_vanilla.predict(X_test)\n      y_test_pred_proba = ppl_vanilla.predict_proba(X_test)[:,1]\n\n      y_train_pred = ppl_vanilla.predict(X_train)\n      y_train_pred_proba = ppl_vanilla.predict_proba(X_train)[:,1]\n\n      std_shrinking_test, std_shrinking_train, \\\n           residual_std_test, y_test_std, residual_std_train, y_train_std, \\\n           accuracy_test, roc_auc_test, pr_auc_test, log_loss_test, mae_test, \\\n           accuracy_train, roc_auc_train, pr_auc_train, log_loss_train, mae_train = eval_model(y_train, y_train_pred, y_train_pred_proba, y_test, y_test_pred, y_test_pred_proba)\n\n      mlflow.set_tag('features', features)\n\n      print(\"Test Log Loss: {0}\".format(log_loss_test))\n      print(\"Train Log Loss: {0}\".format(log_loss_train))\n\n      print(\"Test Accuracy Score: {0}\".format(accuracy_test))\n      print(\"Train Accuracy Score: {0}\".format(accuracy_train))\n\n\n      print(\"Test ROC AUC: {0}\".format(roc_auc_test))\n      print(\"Train ROC AUC: {0}\".format(roc_auc_train))\n\n      print(\"Test PR AUC: {0}\".format(pr_auc_test))\n      print(\"Train PR AUC: {0}\".format(pr_auc_train))\n\n      print(\"Test MAE: {0}\".format(mae_test))\n      print(\"Train MAE: {0}\".format(mae_train))\n\n      mlflow.log_metric('std shrinking', std_shrinking)\n\n      mlflow.log_metric('residual_std', residual_std_test)\n      mlflow.log_metric('y_test_std', y_test_std)\n      mlflow.log_metric('test shrinking', std_shrinking_test)\n\n      mlflow.log_metric('residual_std', residual_std_train)\n      mlflow.log_metric('y_test_std', y_train_std)\n      mlflow.log_metric('test shrinking', std_shrinking_train)\n\n\n      mlflow.log_metric('accuracy_train', accuracy_train)\n      mlflow.log_metric('accuracy_test', accuracy_test)\n\n      mlflow.log_metric('train_pr_auc', pr_auc_train)\n      mlflow.log_metric('test_pr_auc', pr_auc_test)\n\n      mlflow.log_metric('train_roc_auc', roc_auc_train)\n      mlflow.log_metric('test_roc_auc', roc_auc_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x_train shape (8778174, 6) y_train shape (8778174, 1)\nx_test shape (2382516, 6) y_test shape (2382516, 1)\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-bd38da43-724a-4a2e-81c8-430f9301c0ce/lib/python3.5/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-bd38da43-724a-4a2e-81c8-430f9301c0ce/lib/python3.5/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\nin y_train, pct of ones 0.2527589450835675 pct of zeros 0.7472410549164326\nin y_test, pct of ones 0.3067958410352753 pct of zeros 0.6932041589647246\nTest Log Loss: 0.5468556831728841\nTrain Log Loss: 0.47774832758504565\nTest Accuracy Score: 0.7265294335903725\nTrain Accuracy Score: 0.7733612935902159\nTest ROC AUC: 0.7389428222817971\nTrain ROC AUC: 0.7621793339063736\nTest PR AUC: 0.38538475151909163\nTrain PR AUC: 0.3473547932577369\nTest MAE: 0.27347056640962747\nTrain MAE: 0.2266387064097841\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["### Testing"],"metadata":{}},{"cell_type":"code","source":["residual_std_m1, y_std = eval_model(ppl_best, X_test, y_test)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Important features"],"metadata":{}},{"cell_type":"code","source":["dictionary = dict(zip(features, ppl_best['clf'].feature_importances_))\n\ndictionary = {k: v for k, v in sorted(dictionary.items(), key=lambda item: item[1])}\n\n# dictionary = sorted(dictionary.items(), key=operator.itemgetter(1))\nprint(dictionary)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["with open('/dbfs/chizhang/variance_reduction/model/052820_flf_prediction_variance_reduction_ppl.dill', 'wb') as f:\n  dill.dump(ppl_best, f)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["##### END ####\n\n\n\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["## validate correlation in unit level"],"metadata":{}},{"cell_type":"code","source":["with open('/dbfs/chizhang/variance_reduction/model/051620_flf_prediction_variance_reduction_ppl.dill', 'rb') as f:\n  ppl_best = dill.load(f)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["for f in ['store_starting_point_id', 'submarket_id', 'window_id']:\n  print('feature', f)\n  print('\\tTrain PearsonrResult', pearsonr(ppl_best.named_steps[\"target_encode\"].transform(X_test)[f].values, y_test.values.reshape(len(y_test), )))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["df.groupby(['store_starting_point_id', 'daypart']).agg({'is_flf_above_ideal' : 'mean'})"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Model training"],"metadata":{}},{"cell_type":"code","source":["# # features_high_corr = ['STORE_STARTING_POINT_ID', 'SUBMARKET_ID', 'hour_of_day', 'day_of_week'] #better residual std without day_of_week\n\n# features_high_corr = features\n# #model 1: default with balancing\n# clf_default_is_unbalanced = LGBMClassifier(is_unbalance =True)#is_unbalance =True\n# clf_default_is_unbalanced.fit(X_train_encoded[features_high_corr], y_train)\n\n# #model 2: default\n# clf_default = LGBMClassifier()\n# clf_default.fit(X_train_encoded[features_high_corr], y_train)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# # model 3: grid search\n# gridParams = {\n#     'learning_rate': [0.01, 0.05, 0.1],\n#     'num_leaves': [16, 31, 64, 128],\n#     'boosting_type' : ['gbdt', 'dart', 'goss'],\n#     'objective' : ['binary'],\n#     'max_depth' : [-1, 5, 10],\n# #     'random_state' : [501], \n# #     'colsample_bytree' : [0.5,0.7],\n# #     'subsample' : [0.5,0.7],\n# #     'min_split_gain' : [0.01],\n#     'min_data_in_leaf':[5, 10, 20],\n# #     'metric':['auc']\n#     }\n# #{'boosting_type': 'goss', 'objective': 'binary', 'learning_rate': 0.1, 'num_leaves': 64, 'max_depth': -1}\n\n# clf_tuned = LGBMClassifier()\n# grid = GridSearchCV(clf_tuned, gridParams,verbose=2, n_jobs=-1)\n# grid.fit(X_train_encoded[features_high_corr], y_train)\n\n# # print(grid.best_score_)\n# print(grid.best_params_)\n"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# # model 4: random search\n# import scipy.stats as stats\n# randomParams = {\n#     'learning_rate': stats.uniform(0.01, 0.1), \n#     'num_leaves': stats.randint(16, 256),\n#     'boosting_type' : ['gbdt', 'dart', 'goss'],\n#     'objective' : ['binary'],\n#     'max_depth' : [-1, 5, 10],\n# #     'random_state' : [501], \n# #     'colsample_bytree' : [0.5,0.7],\n# #     'subsample' : [0.5,0.7],\n# #     'min_split_gain' : [0.01],\n# #     'min_data_in_leaf':[10],\n# #     'metric':['auc']\n#     }\n# #{'boosting_type': 'goss', 'objective': 'binary', 'learning_rate': 0.1, 'num_leaves': 64, 'max_depth': -1}\n\n# clf_tuned_random = LGBMClassifier()\n# model_random_search = RandomizedSearchCV(clf_tuned_random, randomParams, verbose=2, n_jobs=-1, n_iter=100)\n# model_random_search.fit(X_train_encoded[features_high_corr], y_train)\n\n# # print(grid.best_score_)\n# print(model_random_search.best_params_)\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["## Testing"],"metadata":{}},{"cell_type":"code","source":["residual_std_m1, _ = eval_model(clf_default_is_unbalanced, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["residual_std_m2, _ = eval_model(clf_default, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["residual_std_m3, _ = eval_model(grid, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["residual_std_m4, y_std = eval_model(model_random_search, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["## Result plotting"],"metadata":{}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\nimport matplotlib.ticker as mtick\n\n# plot 4 model performance\n# std_clf_is_unbalanced = 0.4239141851690449\n# std_clf_default = 0.422724398281608\n# std_grid_search = 0.4211761747709921\n# std_random_search = 0.42077421076528226\n# std_random_search_new = 0.4200526230592615 #new features\n# std_testing_data = 0.4555727985659514\n\nstd_testing_data = y_std\n\nstd_clf_is_unbalanced = residual_std_m1\nstd_clf_default = residual_std_m2\nstd_grid_search = residual_std_m3\nstd_random_search = residual_std_m4\n\nheights = [std_testing_data, std_clf_is_unbalanced, std_clf_default, std_grid_search, std_random_search]\nheights =[i for i in heights]\nheights_pct = [1 - i/std_testing_data for i in heights]\nfig, ax = plt.subplots(figsize=(8, 6))\nax.bar(range(len(heights)), heights)\nax.set_ylabel('STD')\n\nplt.xticks(range(len(heights)), ['y std', 'residual_m1 (w/ balancing)', 'residual_m2(w/o balancing)', 'residual_m3(w/ grid search)', 'residual_m4(w/ random search)'])\nplt.xticks(rotation=10)\nax2 = ax.twinx()\nax2.plot(heights_pct, '-o', color='black')\nax2.set_ylabel('% of STD Reduction')\n\nvals = ax2.get_yticks()\nax2.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n\nfor i,j in zip(range(len(heights_pct)), heights_pct):\n    ax2.annotate('{:,.2%}'.format(j),xy=(i,j))\nplt.grid()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["\n\n\n\n\n\n\n\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":66}],"metadata":{"name":"variance_reduction_v5_weather","notebookId":163300},"nbformat":4,"nbformat_minor":0}
