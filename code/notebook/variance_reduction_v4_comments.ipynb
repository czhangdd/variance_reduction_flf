{"cells":[{"cell_type":"code","source":["dbutils.library.installPyPI('snowflake-connector-python', version = \"2.0.2\")\ndbutils.library.installPyPI(\"azure-storage-blob\", version = \"2.1.0\")\ndbutils.library.installPyPI(\"lightgbm\")\n\ndbutils.library.installPyPI(\"scikit-learn\", version=\"0.18.2\")\n\n# dbutils.library.installPyPI('numpy', version=\"1.15.4\")\ndbutils.library.installPyPI('numpy', version=\"1.18.1\")\n\ndbutils.library.install(\"dbfs:/FileStore/jars/dash_ml-1.0.15-py3.6.egg\")\n# dbutils.library.installPyPI(\"dill\", version=\"0.2.7.1\")\ndbutils.library.installPyPI(\"dill\", version=\"0.2.9\")\n\n# dbutils.library.installPyPI(\"pathos\")\ndbutils.library.installPyPI(\"mlflow\")\n# dbutils.library.installPyPI(\"matplotlib\", version = \"1.5.3\")\n# dbutils.library.installPyPI(\"seaborn\")\n# dbutils.library.installPyPI(\"pdpbox\")\n# dbutils.library.installPyPI(\"causalml\")\n# dbutils.library.installPyPI(\"econml\")\ndbutils.library.installPyPI(\"category_encoders\", version=\"2.2.2\")\ndbutils.library.installPyPI(\"pandas\", version=\"0.21.0\")\n\n\ndbutils.library.restartPython()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">PyPI package snowflake-connector-python has been installed already. The previous installed package is PyPI:(snowflake-connector-python)-(2.0.2)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package azure-storage-blob has been installed already. The previous installed package is PyPI:(azure-storage-blob)-(2.1.0)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package lightgbm has been installed already. The previous installed package is PyPI:(lightgbm)-(empty)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package scikit-learn has been installed already. The previous installed package is PyPI:(scikit-learn)-(0.18.2)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package numpy has been installed already. The previous installed package is PyPI:(numpy)-(1.18.1)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nLibrary with this file name dbfs:/FileStore/jars/dash_ml-1.0.15-py3.6.egg already installed. If you would like to reinstall this please detach and reattach to recreate your environment. Conflict previous lib file: dbfs:/FileStore/jars/dash_ml-1.0.15-py3.6.egg\nPyPI package dill has been installed already. The previous installed package is PyPI:(dill)-(0.2.9)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package mlflow has been installed already. The previous installed package is PyPI:(mlflow)-(empty)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package category_encoders has been installed already. The previous installed package is PyPI:(category_encoders)-(2.2.2)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nPyPI package pandas has been installed already. The previous installed package is PyPI:(pandas)-(0.21.0)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["import pandas as pd\nimport os\nfrom os import path\nfrom scipy.stats import pearsonr, spearmanr\nfrom scipy import stats\nimport numpy as np\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.metrics import log_loss, accuracy_score, average_precision_score, confusion_matrix, f1_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport snowflake.connector\nimport dill\nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import TargetEncoder"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["pd.__version__\nimport sklearn\nsklearn.__version__"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>&apos;0.22.2.post1&apos;</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# class TargetEncoder:\n#     def __init__(self, cols=None, n_folds=None):\n#         # default to encoding all categorical columns\n#         if isinstance(cols, str):\n#             self.cols = [cols]\n#         else:\n#             self.cols = cols\n#         self.n_folds = n_folds\n#         self.kfold_means = dict()\n#         self.target_means = dict()\n#         self.fallback = None\n#     def _reg_fit(self, X, y):\n#         # Encode all categorical columns if not specified\n#         if self.cols is None:\n#             cols = []\n#             for i, col in enumerate(X.dtypes):\n#                 if col == 'object':\n#                     cols.append(X.columns[i])\n#             self.cols = cols\n#         if self.n_folds is not None:\n#             self.target_means = dict()\n#         df = X.copy()\n#         df['target'] = y.astype('float')\n#         self.fallback = df['target'].mean()\n#         for cat_col in self.cols:\n#             target = df[[cat_col, 'target']].groupby(cat_col).mean().to_dict()['target']\n#             self.target_means[cat_col] = target\n#         return self\n#     def _reg_transform(self, X, y):\n#         X_transform = X.copy()\n#         cols = []\n#         ls_target_encodings = []\n#         # Likely a way we can impute all the eligible columns without looping\n#         # through each one\n#         for col, val in self.target_means.items():\n#             cols.append(col)\n#             ls_target_encodings.append(\n#                 X_transform[col].map(val).fillna(\n#                     self.fallback))\n#         X_transform[cols] = pd.concat(\n#             ls_target_encodings,\n#             axis='columns',\n#             ignore_index=True)\n#         return X_transform\n#     def _kfold_numbering(self, X, y, n_folds):\n#         # Is there a way I can vectorize this for loop?\n#         df = X.copy()\n#         df['target'] = y.astype('float')\n#         parts = int(len(df) / n_folds)\n#         kfold_df = []\n#         for i in range(n_folds):\n#             if i == n_folds:\n#                 break\n#             else:\n#                 sliced_df = df[i * parts:(i + 1) * parts]\n#                 sliced_df['fold'] = i\n#                 kfold_df.append(sliced_df)\n#         kfold_df = pd.concat(kfold_df)\n#         return kfold_df\n#     def _kfold_fit(self, X, y):\n#         kfold_df = self._kfold_numbering(X, y, self.n_folds)\n#         for i in range(self.n_folds):\n#             # To Do: Figure out how to avoid re-computing these values in a few\n#             # lines of code.\n#             data_for_mean = kfold_df[kfold_df['fold'] != i]\n#             data_for_mean_X = data_for_mean.drop(['target'], axis=1)\n#             data_for_mean_y = data_for_mean['target']\n#             self._reg_fit(data_for_mean_X, data_for_mean_y)\n#             self.kfold_means[i] = self.target_means\n#         return self\n#     def _kfold_transform(self, X, y):\n#         kfold_df = self._kfold_numbering(X, y, self.n_folds)\n#         X_transform = []\n#         for i in self.kfold_means.keys():\n#             data_to_impute = kfold_df[kfold_df['fold'] == i]\n#             data_to_impute_X = data_to_impute.drop(['target'], axis=1)\n#             data_to_impute_y = data_to_impute['target']\n#             # To Do: See if I can map to fold numbers to improve speed\n#             self.target_means = self.kfold_means[i]\n#             fold_transform_X = self._reg_transform(\n#                 data_to_impute_X, data_to_impute_y)\n#             X_transform.append(fold_transform_X)\n#         return pd.concat(X_transform)\n#     def fit(self, X, y):\n#         if self.n_folds is not None:\n#             assert self.n_folds < len(X)/self.n_folds, \"Dataset not large enough to support specified number of folds\"\n#             assert self.n_folds < len(X)/self.n_folds, \"Dataset not large enough to support specified number of folds\"\n#             self._kfold_fit(X, y)\n#         self._reg_fit(X, y)\n#     def transform(self, X, y):\n#         if self.n_folds is not None:\n#             return self._kfold_transform(X, y)\n#         return self._reg_transform(X, y)\n#     def fit_transform(self, X, y):\n#         if self.n_folds is not None:\n#             return self._kfold_fit(X, y, self.n_folds)._kfold_transform(X, y)\n#         return self.fit(X, y).transform(X, y)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def eval_model(model, x_test, y_test):\n  y_pred = model.predict(x_test)\n  y_pred_proba = model.predict_proba(x_test)[:, 1]\n  \n  \n  y_test_reshaped = np.reshape(y_test.astype('float').values, (len(y_test), ))\n  \n  print('in gt, pct of ones', y_test_reshaped.mean(), 'pct of zeros', 1-y_test_reshaped.mean())\n  \n  residual = y_pred_proba - y_test_reshaped\n  print('residual std', residual.std(), 'y_test std', y_test_reshaped.std())\n  \n  logloss_test_pred = log_loss(y_test_reshaped, y_pred_proba)\n#   logloss_baseline = log_loss(y_test_reshaped, np.ones(len(y_pred)))\n  pred_random = np.random.randint(2, size=len(y_test_reshaped))\n  logloss_baseline = log_loss(y_test_reshaped, pred_random)\n  print('logloss_baseline', logloss_baseline)\n  print('logloss_test_pred', logloss_test_pred)\n  \n  acc = accuracy_score(y_test_reshaped, y_pred)\n  f1 = f1_score(y_test_reshaped, y_pred)  \n  roc_auc = roc_auc_score(y_test_reshaped, y_pred_proba)\n  print('acc', acc)\n  print('average_precision_score predicted', average_precision_score(y_test_reshaped, y_pred_proba), 'average_precision_score all zeros', average_precision_score(y_test_reshaped, np.zeros(len(y_pred))))\n  print('f1 score', f1)\n  print('roc_auc_score', roc_auc)\n  conf_max = confusion_matrix(y_test_reshaped, y_pred)/len(y_test_reshaped)\n  print('conf mat', conf_max)\n  \n  return residual.std(), y_test_reshaped.std()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["sql_query_train = \"\"\"\nwith daypart_mapping as (\n  SELECT \n    case when day_part = 'late_night' then 'latenight' \n      else DAY_PART\n    END as daypart,\n    MIN(LOCAL_HOUR) as min_hour,\n    MAX(LOCAL_HOUR) as max_hour\n  FROM PRODDB.STATIC.LOOKUP_DAY_PART_MAPPING \n  GROUP BY 1\n),\nflf_targets as (\nSELECT \n  FLF.STARTING_POINT_ID, \n  FLF.STARTING_POINT_NAME, \n  FLF.TIME_OF_DAY as daypart, \n  dp.min_hour,\n  dp.max_hour,\n  FLF.TARGET_IDEAL_FLF, \n  FLF.MIN_TARGET_FLF_RANGE, \n  FLF.MAX_TARGET_FLF_RANGE, \n  FLF.TARGET_CREATED_AT, \n  LEAD(FLF.TARGET_CREATED_AT, 1) OVER (PARTITION BY FLF.STARTING_POINT_ID, FLF.TIME_OF_DAY ORDER BY FLF.TARGET_CREATED_AT) as next, \n  IFNULL(next, '2022-01-01') AS NEXT_TARGET_CREATED_DATE\nFROM STATIC.LOOKUP_TARGET_FLF_BY_REGION flf\nLEFT JOIN daypart_mapping dp \n  on flf.TIME_OF_DAY = dp.DAYPART\n),\nflf_raw as (\nSELECT\n  dd.created_at,\n  dd.DELIVERY_ID,\n  dd.active_date,\n  dd.STORE_STARTING_POINT_ID,\n  dd.SUBMARKET_ID,\n  dd.flf,\n  fces.num_delivered as num_delivered,\n  fces.num_opened as num_opened,\n  sm.LAUNCH_DATE as submarket_launch_date,\n  convert_timezone('UTC', dd.TIMEZONE, dd.CREATED_AT) as created_at_local,\n  TO_DATE(created_at_local) as created_at_local_date,\n  hour(created_at_local) * 2 + floor(minute(created_at_local)/30.0) as window_id,\n  datediff('second', dd.CREATED_AT, dd.ACTUAL_DELIVERY_TIME)/60.0 as asap,\n  dd.DISTINCT_ACTIVE_DURATION/60.0 as dat,\n  datediff('second', dd.DASHER_CONFIRMED_TIME, dd.DASHER_AT_STORE_TIME)/60.0 as d2r,\n  case when datediff('second', dd.QUOTED_DELIVERY_TIME, dd.ACTUAL_DELIVERY_TIME)/60 > 20 then 1 else 0 END as lateness_20_min,\n  flf.daypart,\n  case when dd.flf - flf.MAX_TARGET_FLF_RANGE > 0 then 1 else 0 END as is_flf_above_max,\n  case when dd.flf - flf.TARGET_IDEAL_FLF > 0 then 1 else 0 END as is_flf_above_ideal\nFROM PRODDB.PUBLIC.DIMENSION_DELIVERIES dd \nLEFT JOIN flf_targets flf\n  on dd.STORE_STARTING_POINT_ID = flf.STARTING_POINT_ID\n  AND hour(convert_timezone('UTC', dd.TIMEZONE, dd.created_at)) between flf.min_hour and flf.max_hour\n  AND convert_timezone('UTC', dd.TIMEZONE, dd.created_at) between flf.TARGET_CREATED_AT and flf.NEXT_TARGET_CREATED_DATE\nLEFT JOIN PRODDB.PUBLIC.MAINDB_SUBMARKET sm \n  ON dd.SUBMARKET_ID = sm.ID\nLEFT JOIN public.fact_cx_email_summary fces\n  ON dd.SUBMARKET_ID = fces.SUBMARKET_ID\n  AND convert_timezone('UTC',dd.timezone,\n dateadd('minute',cast(floor(date_part('minute',dd.created_at) / 30) * 30 as int), date_trunc('hour',dd.created_at))\n ) = fces.half_hour_local\nWHERE dd.created_at between '2020-03-16' and '2020-04-27'\n  AND dd.IS_FILTERED_CORE = true \n  AND dd.IS_ASAP = true \n  AND dd.IS_CONSUMER_PICKUP = false \n  AND fulfillment_type != 'merchant_fleet'\n),\n\nflf_raw_grouped as(\nSELECT\n    t.created_at_local_date,\n    t.DAYPART,\n    t.STORE_STARTING_POINT_ID,\n    AVG(t.ASAP) as avg_asap,\n    AVG(t.DAT) as avg_dat,\n    AVG(t.D2R) as avg_d2r,\n    AVG(t.IS_FLF_ABOVE_IDEAL) as avg_is_flf_above_ideal,\n    AVG(t.flf) as avg_flf,\n    AVG(t.num_opened) as avg_num_opened,\n    AVG(t.num_delivered) as avg_num_delivered\nFROM flf_raw t\nGROUP BY t.created_at_local_date, t.DAYPART, t.STORE_STARTING_POINT_ID\n),\nflf_hist as(\nSELECT \n    t1.created_at_local_date,\n    dayofweek(t1.CREATED_AT_LOCAL) as DAY_OF_WEEK,\n    hour(t1.CREATED_AT_LOCAL) as HOUR_OF_DAY,\n    t1.STORE_STARTING_POINT_ID,\n    t1.SUBMARKET_ID,  \n    t1.DAYPART,\n    t1.WINDOW_ID,\n    t1.is_flf_above_ideal\nFROM flf_raw t1\nLEFT JOIN flf_raw_grouped t2\n    ON t1.created_at_local_date = DATEADD(Day, -15, t2.created_at_local_date)\n    AND t1.DAYPART = t2.DAYPART\n    AND t1.STORE_STARTING_POINT_ID = t2.STORE_STARTING_POINT_ID\n)\n\nSELECT *\nFROM flf_hist\nSAMPLE(20)\n\"\"\"\n\n# sql_query_test_verify = \"\"\"\n# CREATE TEMP TABLE CHIZHANG.DAYPART_MAPPING AS (\n#   SELECT \n#     CASE WHEN day_part = 'late_night' THEN 'latenight' \n#       ELSE DAY_PART\n#     END AS daypart,\n#     MIN(LOCAL_HOUR) AS min_hour,\n#     MAX(LOCAL_HOUR) AS max_hour\n#   FROM PRODDB.STATIC.LOOKUP_DAY_PART_MAPPING \n#   GROUP BY 1\n# );\n\n# CREATE temp TABLE CHIZHANG.FLF_TARGETS AS (\n# SELECT \n#   FLF.STARTING_POINT_ID, \n#   FLF.STARTING_POINT_NAME, \n#   FLF.TIME_OF_DAY AS daypart, \n#   dp.min_hour,\n#   dp.max_hour,\n#   FLF.TARGET_IDEAL_FLF, \n#   FLF.MIN_TARGET_FLF_RANGE, \n#   FLF.MAX_TARGET_FLF_RANGE, \n#   FLF.TARGET_CREATED_AT, \n#   LEAD(FLF.TARGET_CREATED_AT, 1) OVER (PARTITION BY FLF.STARTING_POINT_ID, FLF.TIME_OF_DAY ORDER BY FLF.TARGET_CREATED_AT) AS NEXT, \n#   IFNULL(NEXT, '2022-01-01') AS NEXT_TARGET_CREATED_DATE\n# FROM STATIC.LOOKUP_TARGET_FLF_BY_REGION flf\n# LEFT JOIN CHIZHANG.DAYPART_MAPPING dp \n#   ON flf.TIME_OF_DAY = dp.DAYPART\n# );\n    \n# CREATE temp TABLE CHIZHANG.FLF_RAW AS (\n# SELECT\n#   dd.created_at,\n#   dd.DELIVERY_ID,\n#   dd.active_date,\n#   dd.STORE_STARTING_POINT_ID,\n#   dd.SUBMARKET_ID,\n#   dd.flf,\n#   fces.num_delivered AS num_delivered,\n#   fces.num_opened AS num_opened,\n#   sm.LAUNCH_DATE AS submarket_launch_date,\n#   convert_timezone('UTC', dd.TIMEZONE, dd.CREATED_AT) AS created_at_local,\n#   TO_DATE(created_at_local) AS created_at_local_date,\n#   hour(created_at_local) * 2 + floor(minute(created_at_local)/30.0) AS window_id,\n#   datediff('second', dd.CREATED_AT, dd.ACTUAL_DELIVERY_TIME)/60.0 AS asap,\n#   dd.DISTINCT_ACTIVE_DURATION/60.0 AS dat,\n#   datediff('second', dd.DASHER_CONFIRMED_TIME, dd.DASHER_AT_STORE_TIME)/60.0 as d2r,\n#   CASE WHEN datediff('second', dd.QUOTED_DELIVERY_TIME, dd.ACTUAL_DELIVERY_TIME)/60 > 20 THEN 1 ELSE 0 END AS lateness_20_min,\n#   flf.daypart,\n#   CASE WHEN dd.flf - flf.MAX_TARGET_FLF_RANGE > 0 THEN 1 ELSE 0 END AS is_flf_above_max,\n#   CASE WHEN dd.flf - flf.TARGET_IDEAL_FLF > 0 THEN 1 ELSE 0 END AS is_flf_above_ideal\n# FROM PRODDB.PUBLIC.DIMENSION_DELIVERIES dd \n# LEFT JOIN CHIZHANG.FLF_TARGETS flf\n#   ON dd.STORE_STARTING_POINT_ID = flf.STARTING_POINT_ID\n#   AND hour(convert_timezone('UTC', dd.TIMEZONE, dd.created_at)) BETWEEN flf.min_hour AND flf.max_hour\n#   AND convert_timezone('UTC', dd.TIMEZONE, dd.created_at) BETWEEN flf.TARGET_CREATED_AT AND flf.NEXT_TARGET_CREATED_DATE\n# LEFT JOIN PRODDB.PUBLIC.MAINDB_SUBMARKET sm \n#   ON dd.SUBMARKET_ID = sm.ID\n# LEFT JOIN public.fact_cx_email_summary fces\n#   ON dd.SUBMARKET_ID = fces.SUBMARKET_ID\n#   AND convert_timezone('UTC',dd.timezone,\n#  dateadd('minute',CAST(floor(date_part('minute',dd.created_at) / 30) * 30 AS INT), date_trunc('hour',dd.created_at))\n#  ) = fces.half_hour_local\n# //  WHERE CAST(DD.CREATED_AT as DATE) >= dateadd('DAY', -3, TO_TIMESTAMP_NTZ(LOCALTIMESTAMP)) \n#   WHERE dd.created_at BETWEEN '2020-03-16' AND '2020-04-27'\n#   AND CAST(DD.CREATED_AT as DATE) < dateadd('DAY', 0, TO_TIMESTAMP_NTZ(LOCALTIMESTAMP))  \n#   AND dd.IS_FILTERED_CORE = true \n#   AND dd.IS_ASAP = true \n#   AND dd.IS_CONSUMER_PICKUP = false \n#   AND fulfillment_type != 'merchant_fleet'\n# );\n\n# CREATE TEMP TABLE CHIZHANG.FLF_RAW_GROUPED AS(\n# SELECT\n#     t.created_at_local_date,\n#     t.DAYPART,\n#     t.STORE_STARTING_POINT_ID,\n#     AVG(t.ASAP) AS avg_asap,\n#     AVG(t.DAT) AS avg_dat,\n#     AVG(t.D2R) AS avg_d2r,\n#     AVG(t.IS_FLF_ABOVE_IDEAL) AS avg_is_flf_above_ideal,\n#     AVG(t.flf) AS avg_flf,\n#     AVG(t.num_opened) AS avg_num_opened,\n#     AVG(t.num_delivered) AS avg_num_delivered\n# FROM CHIZHANG.FLF_RAW t\n# GROUP BY t.created_at_local_date, t.DAYPART, t.STORE_STARTING_POINT_ID\n# );\n\n# SELECT \n#     -- t1.CREATED_AT,\n#     -- t1.DELIVERY_ID, \n#     -- t1.ACTIVE_DATE, \n#     dayofweek(t1.CREATED_AT_LOCAL) as DAY_OF_WEEK,\n#     hour(t1.CREATED_AT_LOCAL) as HOUR_OF_DAY,\n#     t1.STORE_STARTING_POINT_ID,\n#     t1.SUBMARKET_ID,\n#     -- t1.FLF,\n#     -- t1.NUM_DELIVERED, \n#     -- t1.NUM_OPENED,\n#     -- t1.SUBMARKET_LAUNCH_DATE, \n#     -- t1.CREATED_AT_LOCAL, \n#     -- t1.CREATED_AT_LOCAL_DATE,\n#     -- t1.ASAP, \n#     -- t1.DAT,\n#     -- t1.D2R, \n#     -- t1.LATENESS_20_MIN,\n#     t1.DAYPART,\n#     t1.WINDOW_ID\n#     -- t1.IS_FLF_ABOVE_MAX, \n#     -- t1.IS_FLF_ABOVE_IDEAL,\n#     -- t2.avg_asap,\n#     -- t2.avg_dat,\n#     -- t2.avg_d2r,\n#     -- t2.avg_is_flf_above_ideal,\n#     -- t2.avg_flf,\n#     -- t2.avg_num_delivered,\n#     -- t2.avg_num_opened\n# FROM CHIZHANG.FLF_RAW t1\n# LEFT JOIN CHIZHANG.FLF_RAW_GROUPED t2\n#     ON t1.created_at_local_date = DATEADD(Day, -15, t2.created_at_local_date)\n#     AND t1.DAYPART = t2.DAYPART\n#     AND t1.STORE_STARTING_POINT_ID = t2.STORE_STARTING_POINT_ID\n# SAMPLE(5)\n# \"\"\"\nuser = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-user\")\npassword = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-password\")\n\nos.environ['SNOWFLAKE_USER'] = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-user\")\nos.environ['SNOWFLAKE_PW'] = dbutils.secrets.get(scope=\"chizhang-scope\", key=\"snowflake-password\")\n\n# snowflake connection options\nparams = dict(\n  user=os.environ['SNOWFLAKE_USER'],\n  password=os.environ['SNOWFLAKE_PW'],\n  account='DOORDASH',\n  database='PRODDB',\n  warehouse='ADHOC',\n  schema='public',\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Data loading"],"metadata":{}},{"cell_type":"code","source":["scope_name = 'chizhang-scope'\npw_key_name = 'snowflake-password'\nun_key_name = 'snowflake-user'\nuser = dbutils.secrets.get(scope=scope_name, key=un_key_name)\npassword = dbutils.secrets.get(scope=scope_name, key=pw_key_name)\n\n# snowflake connection options\noptions = dict(sfurl=\"doordash.snowflakecomputing.com/\",\n               sfaccount=\"DOORDASH\",\n               sfuser=user,\n               sfpassword=password,\n               sfdatabase=\"PRODDB\",\n               sfschema=\"public\",\n               sfwarehouse=\"ADHOC\")\n\nprint(user)\nprint(password)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[REDACTED]\n[REDACTED]\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["sql_load_table = \"\"\" select * from variance_reduction_flf_train_test \"\"\"\nall_data = spark.read.format(\"snowflake\").options(**options).option(\"query\", sql_load_table).load()\nraw_data = all_data.toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# filename = 'percentage.csv'\n# filename = 'small.csv'\n# filename = 'data_new_agg_feat_sample20.csv'\n# filename = 'variation_reduction_new_feat_sample20.csv'\n\n# dbutils.fs.cp('dbfs:/chizhang/variance_reduction/data/{}'.format(filename), 'file:/databricks/driver/{}'.format(filename))\n# raw_data = pd.read_csv(filename)\n# sql_query = \"select * from CHIZHANG.VARIANCE_REDUCTION_FLF\"\n\nsql_query = sql_query_train\nwith snowflake.connector.connect(**params) as ctx:\n  raw_data = pd.read_sql(sql_query, ctx)\n  raw_data.to_csv('/dbfs/chizhang/variance_reduction/data/data_new_agg_feat_sample20_post_covid.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["df = raw_data.copy()\ndf = df.reindex()\nprint('data size', df.shape)\ndf.columns = map(str.lower, df.columns)\ndf.head(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1500px;overflow:auto;\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at_local_date</th>\n      <th>day_of_week</th>\n      <th>hour_of_day</th>\n      <th>store_starting_point_id</th>\n      <th>submarket_id</th>\n      <th>daypart</th>\n      <th>window_id</th>\n      <th>is_flf_above_ideal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-04-06</td>\n      <td>1</td>\n      <td>9</td>\n      <td>254</td>\n      <td>9</td>\n      <td>breakfast</td>\n      <td>18</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-04-06</td>\n      <td>1</td>\n      <td>9</td>\n      <td>255</td>\n      <td>9</td>\n      <td>breakfast</td>\n      <td>18</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-04-06</td>\n      <td>1</td>\n      <td>9</td>\n      <td>255</td>\n      <td>9</td>\n      <td>breakfast</td>\n      <td>18</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-04-06</td>\n      <td>1</td>\n      <td>9</td>\n      <td>255</td>\n      <td>9</td>\n      <td>breakfast</td>\n      <td>18</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-04-06</td>\n      <td>1</td>\n      <td>9</td>\n      <td>254</td>\n      <td>9</td>\n      <td>breakfast</td>\n      <td>18</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Feature creating"],"metadata":{}},{"cell_type":"code","source":["# combine real-time features with historical aggregated features\n# features_cat = ['day_of_week', 'hour_of_day', 'STORE_STARTING_POINT_ID', 'SUBMARKET_ID', 'LATENESS_20_MIN', 'DAYPART', '30min'] #30 min, sp, submarket, ->market level feats\n# historical aggregated features := avg(feat_values) over SP_ID/(DATE_LOCAL-15days)/DAYPART units (i.e., 14 days ago) \n# features_num = ['AVG_ASAP', 'AVG_DAT', 'AVG_D2R', 'AVG_FLF', 'AVG_IS_FLF_ABOVE_IDEAL']\n\nfeatures_cat = ['day_of_week', 'hour_of_day', 'store_starting_point_id', 'submarket_id', 'daypart', 'window_id'] #30 min, sp, submarket, ->market level feats\n# features_num = ['AVG_ASAP', 'AVG_DAT', 'AVG_D2R', 'AVG_FLF', 'AVG_IS_FLF_ABOVE_IDEAL', 'AVG_NUM_DELIVERED', 'AVG_NUM_OPENED']\nfeatures_num = []\n\nfeatures = features_cat + features_num\nmetrics = ['is_flf_above_ideal']\n\nfor f in features_cat:\n  df[f] = df[f].astype('category')\nfor f in features_num:\n  df[f] = df[f].astype('float')\n  \nX = df[features]\ny = df[metrics]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["## Train/test data preparing"],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime, timedelta\n\ndf['created_at_local_date'] = pd.to_datetime(df['created_at_local_date'])\ndate_train_end = df['created_at_local_date'].max() - timedelta(weeks=1)\ndate_train_start = date_train_end - timedelta(weeks=5)\n\ntrain_index = df[(df['created_at_local_date'] < str(date_train_end)) & (df['created_at_local_date'] >= str(date_train_start))].index\ntest_index = df[df['created_at_local_date'] >= str(date_train_end)].index\n\nprint('training period:', df.loc[train_index]['created_at_local_date'].min(), df.loc[train_index]['created_at_local_date'].max())\nprint('testing period:', df.loc[test_index]['created_at_local_date'].min(), df.loc[test_index]['created_at_local_date'].max())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">training period: 2020-03-16 00:00:00 2020-04-19 00:00:00\ntesting period: 2020-04-20 00:00:00 2020-04-27 00:00:00\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# check train and test index do not intersects\nassert len(train_index.intersection(test_index))==0, 'data leakage'\n# check if training and testing data has 7 days a week\nassert df.loc[test_index]['day_of_week'].nunique() == 7, 'testing data does not have 7 days a week'\nassert df.loc[train_index]['day_of_week'].nunique() == 7, 'training data does not have 7 days a week'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["X_train = X.loc[train_index]\ny_train = y.loc[train_index]\n\nX_test = X.loc[test_index]\ny_test = y.loc[test_index]\n\nprint(\"x_train shape\", X_train.shape, 'y_train shape', y_train.shape)\nprint(\"x_test shape\", X_test.shape, 'y_test shape', y_test.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x_train shape (8775841, 6) y_train shape (8775841, 1)\nx_test shape (2384570, 6) y_test shape (2384570, 1)\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Create pipeline & train"],"metadata":{}},{"cell_type":"code","source":["# target encoding\nteppl = TargetEncoder(cols=['store_starting_point_id', 'submarket_id', 'window_id'])\n# clf\nclfppl = LGBMClassifier()\n\n# random search\nrandomParams = {\n    'clf__learning_rate': stats.uniform(0.01, 0.1), \n    'clf__num_leaves': stats.randint(16, 512),\n    'clf__boosting_type' : ['gbdt', 'dart', 'goss']    \n}\n\nppl = Pipeline([('target_encode', teppl), ('clf', clfppl)])\n\nrandom_search = RandomizedSearchCV(ppl, randomParams, n_jobs=-1, verbose=2, n_iter=100)\nrandom_search.fit(X_train, y_train)\n\n\n# ppl = pipeline.set_params(clfppl__learning_rate = random_search.best_params_['learning_rate'],\n#                           clfppl__num_leaves = random_search.best_params_['num_leaves'],\n#                           clfppl__boosting_type = random_search.best_params_['boosting_type']\n#                           )\n    \n# ppl.fit(X_train, y_train)\n    \n\n# clf_random_search.fit(X_train, y_train)\n# clfppl = clf_random_search.best_estimator_\n\n# # pack encoder and best clf model into a pipeline\n# ppl = Pipeline([('target_encode', teppl), ('lightgbm', clfppl)])\n# ppl.set_params(target_encode__cols=['store_starting_point_id', 'submarket_id', 'window_id'],\n#               lightgbm__learning_rate = stats.uniform(0.01, 0.1), \n#               lightgbm__num_leaves = stats.randint(16, 256),\n#               lightgbm__boosting_type = ['gbdt', 'dart', 'goss']).fit(X_train, y_train)\n\n# ppl.fit(X_train, y_train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fitting 5 folds for each of 100 candidates, totalling 500 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 64 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  6.8min\n[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed: 30.4min\n[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 59.3min finished\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-396fa41b-568b-453c-aaca-dad4d2b73d4a/lib/python3.5/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-396fa41b-568b-453c-aaca-dad4d2b73d4a/lib/python3.5/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n<span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>RandomizedSearchCV(cv=None, error_score=nan,\n                   estimator=Pipeline(memory=None,\n                                      steps=[(&apos;target_encode&apos;,\n                                              TargetEncoder(cols=[&apos;store_starting_point_id&apos;,\n                                                                  &apos;submarket_id&apos;,\n                                                                  &apos;window_id&apos;],\n                                                            drop_invariant=False,\n                                                            handle_missing=&apos;value&apos;,\n                                                            handle_unknown=&apos;value&apos;,\n                                                            min_samples_leaf=1,\n                                                            return_df=True,\n                                                            smoothing=1.0,\n                                                            verbose=0)),\n                                             (&apos;clf&apos;,\n                                              LGBMClassifier(boosting_type=&apos;gbdt&apos;,\n                                                             class_we...\n                   iid=&apos;deprecated&apos;, n_iter=100, n_jobs=-1,\n                   param_distributions={&apos;clf__boosting_type&apos;: [&apos;gbdt&apos;, &apos;dart&apos;,\n                                                               &apos;goss&apos;],\n                                        &apos;clf__learning_rate&apos;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3bdbe455c0&gt;,\n                                        &apos;clf__num_leaves&apos;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3bdbe45cf8&gt;},\n                   pre_dispatch=&apos;2*n_jobs&apos;, random_state=None, refit=True,\n                   return_train_score=False, scoring=None, verbose=2)</div>"]}}],"execution_count":19},{"cell_type":"code","source":["random_search.best_params_"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>{&apos;clf__learning_rate&apos;: 0.06663703478387659,\n &apos;clf__boosting_type&apos;: &apos;gbdt&apos;,\n &apos;clf__num_leaves&apos;: 455}</div>"]}}],"execution_count":20},{"cell_type":"code","source":["ppl_best = random_search.best_estimator_"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["### Testing"],"metadata":{}},{"cell_type":"code","source":["residual_std_m1, _ = eval_model(ppl_best, X_test, y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">in gt, pct of ones 0.3075288207098135 pct of zeros 0.6924711792901865\nresidual std 0.4226332599530733 y_test std 0.46147030797511224\nlogloss_baseline 17.272199785921817\nlogloss_test_pred 0.5429674792187813\nacc 0.7293285581886881\naverage_precision_score predicted 0.5639626780596179 average_precision_score all zeros 0.3075288207098135\nf1 score 0.34792360188520083\nroc_auc_score 0.7466992954620041\nconf mat [[0.65711847 0.03535271]\n [0.23531874 0.07221008]]\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Eval raw correlation"],"metadata":{}},{"cell_type":"code","source":["for f in ['store_starting_point_id', 'submarket_id', 'window_id']:\n  print('feature', f)\n  print('\\tTrain PearsonrResult', pearsonr(ppl_best.named_steps[\"target_encode\"].transform(X_train)[f].values, y_train.values.reshape(len(y_train), )))\n#   print('\\tTrain PearsonrResult', pearsonr(X_train_encoded[f].values, y_train.values.reshape(len(y_train), )))#, spearmanr(X_train_encoded[f], y_train))  \n#   print('\\tTest PearsonrResult', pearsonr(X_test_encoded[f].values, y_test.values.reshape(len(y_test), )))#, spearmanr(X_test_encoded[f], y_test))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">feature store_starting_point_id\n\tTrain PearsonrResult (0.309752439357013, 0.0)\nfeature submarket_id\n\tTrain PearsonrResult (0.2405288876488967, 0.0)\nfeature window_id\n\tTrain PearsonrResult (0.1947056284116564, 0.0)\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["with open('/dbfs/chizhang/variance_reduction/model/051620_flf_prediction_variance_reduction_ppl.dill', 'wb') as f:\n  dill.dump(ppl_best, f)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["##### END ####\n\n\n\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["## validate correlation in unit level"],"metadata":{}},{"cell_type":"code","source":["with open('/dbfs/chizhang/variance_reduction/model/051620_flf_prediction_variance_reduction_ppl.dill', 'rb') as f:\n  ppl_best = dill.load(f)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["for f in ['store_starting_point_id', 'submarket_id', 'window_id']:\n  print('feature', f)\n  print('\\tTrain PearsonrResult', pearsonr(ppl_best.named_steps[\"target_encode\"].transform(X_test)[f].values, y_test.values.reshape(len(y_test), )))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">feature store_starting_point_id\n\tTrain PearsonrResult (0.3165188526792855, 0.0)\nfeature submarket_id\n\tTrain PearsonrResult (0.2388020573006394, 0.0)\nfeature window_id\n\tTrain PearsonrResult (0.23488981247182145, 0.0)\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df.groupby(['store_starting_point_id', 'daypart']).agg({'is_flf_above_ideal' : 'mean'})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1500px;overflow:auto;\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>is_flf_above_ideal</th>\n    </tr>\n    <tr>\n      <th>store_starting_point_id</th>\n      <th>daypart</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">6</th>\n      <th>breakfast</th>\n      <td>0.339152</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.396425</td>\n    </tr>\n    <tr>\n      <th>early_morning</th>\n      <td>0.201550</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.036980</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.356063</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.197766</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">9</th>\n      <th>breakfast</th>\n      <td>0.073864</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.191677</td>\n    </tr>\n    <tr>\n      <th>early_morning</th>\n      <td>0.033592</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.034109</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.006754</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.088283</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">10</th>\n      <th>breakfast</th>\n      <td>0.267241</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.380922</td>\n    </tr>\n    <tr>\n      <th>early_morning</th>\n      <td>0.229508</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.048673</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.387824</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.101695</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">11</th>\n      <th>breakfast</th>\n      <td>0.085427</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.389990</td>\n    </tr>\n    <tr>\n      <th>early_morning</th>\n      <td>0.231579</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.036257</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.106682</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.182515</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">12</th>\n      <th>breakfast</th>\n      <td>0.285470</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.569812</td>\n    </tr>\n    <tr>\n      <th>early_morning</th>\n      <td>0.118012</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.044199</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.344269</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.164179</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7845</th>\n      <th>snack</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">7846</th>\n      <th>breakfast</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">7877</th>\n      <th>breakfast</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.164179</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">7910</th>\n      <th>breakfast</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">7943</th>\n      <th>breakfast</th>\n      <td>0.236842</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.168675</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.182796</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.171429</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">7976</th>\n      <th>breakfast</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">8011</th>\n      <th>breakfast</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>dinner</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>latenight</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>lunch</th>\n      <td>0.034483</td>\n    </tr>\n    <tr>\n      <th>snack</th>\n      <td>0.103448</td>\n    </tr>\n  </tbody>\n</table>\n<p>10669 rows × 1 columns</p>\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["## Model training"],"metadata":{}},{"cell_type":"code","source":["# # features_high_corr = ['STORE_STARTING_POINT_ID', 'SUBMARKET_ID', 'hour_of_day', 'day_of_week'] #better residual std without day_of_week\n\n# features_high_corr = features\n# #model 1: default with balancing\n# clf_default_is_unbalanced = LGBMClassifier(is_unbalance =True)#is_unbalance =True\n# clf_default_is_unbalanced.fit(X_train_encoded[features_high_corr], y_train)\n\n# #model 2: default\n# clf_default = LGBMClassifier()\n# clf_default.fit(X_train_encoded[features_high_corr], y_train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["# # model 3: grid search\n# gridParams = {\n#     'learning_rate': [0.01, 0.05, 0.1],\n#     'num_leaves': [16, 31, 64, 128],\n#     'boosting_type' : ['gbdt', 'dart', 'goss'],\n#     'objective' : ['binary'],\n#     'max_depth' : [-1, 5, 10],\n# #     'random_state' : [501], \n# #     'colsample_bytree' : [0.5,0.7],\n# #     'subsample' : [0.5,0.7],\n# #     'min_split_gain' : [0.01],\n#     'min_data_in_leaf':[5, 10, 20],\n# #     'metric':['auc']\n#     }\n# #{'boosting_type': 'goss', 'objective': 'binary', 'learning_rate': 0.1, 'num_leaves': 64, 'max_depth': -1}\n\n# clf_tuned = LGBMClassifier()\n# grid = GridSearchCV(clf_tuned, gridParams,verbose=2, n_jobs=-1)\n# grid.fit(X_train_encoded[features_high_corr], y_train)\n\n# # print(grid.best_score_)\n# print(grid.best_params_)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"code","source":["# # model 4: random search\n# import scipy.stats as stats\n# randomParams = {\n#     'learning_rate': stats.uniform(0.01, 0.1), \n#     'num_leaves': stats.randint(16, 256),\n#     'boosting_type' : ['gbdt', 'dart', 'goss'],\n#     'objective' : ['binary'],\n#     'max_depth' : [-1, 5, 10],\n# #     'random_state' : [501], \n# #     'colsample_bytree' : [0.5,0.7],\n# #     'subsample' : [0.5,0.7],\n# #     'min_split_gain' : [0.01],\n# #     'min_data_in_leaf':[10],\n# #     'metric':['auc']\n#     }\n# #{'boosting_type': 'goss', 'objective': 'binary', 'learning_rate': 0.1, 'num_leaves': 64, 'max_depth': -1}\n\n# clf_tuned_random = LGBMClassifier()\n# model_random_search = RandomizedSearchCV(clf_tuned_random, randomParams, verbose=2, n_jobs=-1, n_iter=100)\n# model_random_search.fit(X_train_encoded[features_high_corr], y_train)\n\n# # print(grid.best_score_)\n# print(model_random_search.best_params_)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["## Testing"],"metadata":{}},{"cell_type":"code","source":["residual_std_m1, _ = eval_model(clf_default_is_unbalanced, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-148891&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>residual_std_m1<span class=\"ansiyellow\">,</span> _ <span class=\"ansiyellow\">=</span> eval_model<span class=\"ansiyellow\">(</span>clf_default_is_unbalanced<span class=\"ansiyellow\">,</span> X_test_encoded<span class=\"ansiyellow\">[</span>features_high_corr<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> y_test<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;clf_default_is_unbalanced&apos; is not defined</div>"]}}],"execution_count":38},{"cell_type":"code","source":["residual_std_m2, _ = eval_model(clf_default, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["residual_std_m3, _ = eval_model(grid, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["residual_std_m4, y_std = eval_model(model_random_search, X_test_encoded[features_high_corr], y_test)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## Result plotting"],"metadata":{}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\nimport matplotlib.ticker as mtick\n\n# plot 4 model performance\n# std_clf_is_unbalanced = 0.4239141851690449\n# std_clf_default = 0.422724398281608\n# std_grid_search = 0.4211761747709921\n# std_random_search = 0.42077421076528226\n# std_random_search_new = 0.4200526230592615 #new features\n# std_testing_data = 0.4555727985659514\n\nstd_testing_data = y_std\n\nstd_clf_is_unbalanced = residual_std_m1\nstd_clf_default = residual_std_m2\nstd_grid_search = residual_std_m3\nstd_random_search = residual_std_m4\n\nheights = [std_testing_data, std_clf_is_unbalanced, std_clf_default, std_grid_search, std_random_search]\nheights =[i for i in heights]\nheights_pct = [1 - i/std_testing_data for i in heights]\nfig, ax = plt.subplots(figsize=(8, 6))\nax.bar(range(len(heights)), heights)\nax.set_ylabel('STD')\n\nplt.xticks(range(len(heights)), ['y std', 'residual_m1 (w/ balancing)', 'residual_m2(w/o balancing)', 'residual_m3(w/ grid search)', 'residual_m4(w/ random search)'])\nplt.xticks(rotation=10)\nax2 = ax.twinx()\nax2.plot(heights_pct, '-o', color='black')\nax2.set_ylabel('% of STD Reduction')\n\nvals = ax2.get_yticks()\nax2.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n\nfor i,j in zip(range(len(heights_pct)), heights_pct):\n    ax2.annotate('{:,.2%}'.format(j),xy=(i,j))\nplt.grid()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["\n\n\n\n\n\n\n\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":45}],"metadata":{"name":"variance_reduction_v4_comments","notebookId":148861},"nbformat":4,"nbformat_minor":0}
